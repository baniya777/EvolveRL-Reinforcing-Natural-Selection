{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74281a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0687e90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    }
   ],
   "source": [
    "text1 = 'prey_1'\n",
    "text2 = 'predator_1'\n",
    "print(text1[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205d163b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e80cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our gym environment \n",
    "env = gym.make(\"CartPole-v1\",render_mode = \"human\")\n",
    "# We get the shape of a state and the actions space size\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520480af",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.n_actions = action_size\n",
    "        # we define some parameters and hyperparameters:\n",
    "        # \"lr\" : learning rate\n",
    "        # \"gamma\": discounted factor\n",
    "        # \"exploration_proba_decay\": decay of the exploration probability\n",
    "        # \"batch_size\": size of experiences we sample to train the DNN\n",
    "        self.lr = 0.001\n",
    "        self.gamma = 0.99\n",
    "        self.exploration_proba = 1.0\n",
    "        self.exploration_proba_decay = 0.005\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # We define our memory buffer where we will store our experiences\n",
    "        # We stores only the 2000 last time steps\n",
    "        self.memory_buffer= list()\n",
    "        self.max_memory_buffer = 2000\n",
    "        \n",
    "        # We creaate our model having to hidden layers of 24 units (neurones)\n",
    "        # The first layer has the same size as a state size\n",
    "        # The last layer has the size of actions space\n",
    "        self.model = Sequential([\n",
    "            Dense(units=24,input_dim=state_size, activation = 'relu'),\n",
    "            Dense(units=24,activation = 'relu'),\n",
    "            Dense(units=24,activation = 'relu'),\n",
    "            Dense(units=action_size, activation = 'linear')\n",
    "        ])\n",
    "        self.model.compile(loss=\"mse\",\n",
    "                      optimizer = Adam(lr=self.lr))\n",
    "        \n",
    "    # The agent computes the action to perform given a state \n",
    "    def compute_action(self, current_state):\n",
    "        # We sample a variable uniformly over [0,1]\n",
    "        # if the variable is less than the exploration probability\n",
    "        #     we choose an action randomly\n",
    "        # else\n",
    "        #     we forward the state through the DNN and choose the action \n",
    "        #     with the highest Q-value.\n",
    "        if np.random.uniform(0,1) < self.exploration_proba:\n",
    "            return np.random.choice(range(self.n_actions))\n",
    "        q_values = self.model.predict(current_state)[0]\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    # when an episode is finished, we update the exploration probability using \n",
    "    # espilon greedy algorithm\n",
    "    def update_exploration_probability(self):\n",
    "        self.exploration_proba = self.exploration_proba * np.exp(-self.exploration_proba_decay)\n",
    "        print(self.exploration_proba)\n",
    "    \n",
    "    # At each time step, we store the corresponding experience\n",
    "    def store_episode(self,current_state, action, reward, next_state, done):\n",
    "        #We use a dictionnary to store them\n",
    "        self.memory_buffer.append({\n",
    "            \"current_state\":current_state,\n",
    "            \"action\":action,\n",
    "            \"reward\":reward,\n",
    "            \"next_state\":next_state,\n",
    "            \"done\" :done\n",
    "        })\n",
    "        # If the size of memory buffer exceeds its maximum, we remove the oldest experience\n",
    "        if len(self.memory_buffer) > self.max_memory_buffer:\n",
    "            self.memory_buffer.pop(0)\n",
    "    \n",
    "\n",
    "    # At the end of each episode, we train our model\n",
    "    def train(self):\n",
    "        # We shuffle the memory buffer and select a batch size of experiences\n",
    "        np.random.shuffle(self.memory_buffer)\n",
    "        batch_sample = self.memory_buffer[0:self.batch_size]\n",
    "        \n",
    "        # We iterate over the selected experiences\n",
    "        for experience in batch_sample:\n",
    "            # We compute the Q-values of S_t\n",
    "            q_current_state = self.model.predict(experience[\"current_state\"])\n",
    "            # We compute the Q-target using Bellman optimality equation\n",
    "            q_target = experience[\"reward\"]\n",
    "            if not experience[\"done\"]:\n",
    "                q_target = q_target + self.gamma*np.max(self.model.predict(experience[\"next_state\"])[0])\n",
    "            q_current_state[0][experience[\"action\"]] = q_target\n",
    "            # train the model\n",
    "            self.model.fit(experience[\"current_state\"], q_current_state, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
